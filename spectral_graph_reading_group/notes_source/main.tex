\documentclass[a4paper]{article}
\usepackage{geometry}
\geometry{left=2.5cm, right=2.5cm, top=2.0cm, bottom=2.0cm}
\setlength{\parskip}{\baselineskip}%
\setlength\parindent{0pt}
\usepackage[utf8]{inputenc}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsthm}

\usepackage{mathtools}
\DeclarePairedDelimiter\floor{\lfloor}{\rfloor}

\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{lemma}{Lemma}[section]

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}

\usepackage{graphicx}

\title{Notes on Spectral Graph Theory}
\author{Mufei Li}
\date{October 2019}

\begin{document}

\maketitle

This notes is based on the reading group on spectral graph theory at AWS Shanghai AI Lab, Fall 2019. Our primary reference are~\cite{Luca_notes_2016} and~\cite{Spielman_notes_2018}, accompanied by~\cite{Luxburg07}. Some sections have not been covered directly in the reading group and I mark the titles of them with an $\ast$.

\tableofcontents

\section{Background}

This section is mainly for a preparation and a refresh of the memory. You can skip it first and go back if necessary.

\subsection{Linear Algebra}

\begin{theorem}[Variational Characterization of Eigenvalues]
\label{thm:var_char}
Let $M\in\mathbb{R}^{n\times n}$ be a symmetric matrix, and $\lambda_1\leq \lambda_2\leq \cdots \leq \lambda_n$ be the eigenvalues of $M$. Then
\begin{equation}
    \lambda_k = \min_{k\text{-dim} V}\max_{\textbf{x}\in V\setminus\{\textbf{0}\}}\frac{\textbf{x}^T M\textbf{x}}{\textbf{x}^T\textbf{x}}
\end{equation}
The quantity $\frac{\textbf{x}^T M\textbf{x}}{\textbf{x}^T\textbf{x}}$ is called the \textit{Rayleigh quotient} of \textbf{x} with respect to $M$, and we will denote it by $R_M(\textbf{x)}$.
\end{theorem}

\begin{proof}
By the spectral theorem, let $\textbf{v}_1, \cdots, \textbf{v}_n$ be eigenvectors of $M$ corresponding to eigenvalues $\lambda_1, \cdots, \lambda_n$ that form an orthonormal basis of $\mathbb{R}^n$. $\forall \textbf{x}\in \mathbb{R}^{n}\setminus\{\textbf{0}\}, \textbf{x}=\sum_{i=1}^{n}a_i\textbf{v}_i$, $\textbf{x}^{T}\textbf{x}=\sum_{i=1}^{n}a_i^2, \textbf{x}^{T}M\textbf{x}=\sum_{i=1}^{n}\lambda_i a_i^2$.

Consider $V$ a space spanned by $\textbf{v}_1,\cdots, \textbf{v}_k$. It follows that $\textbf{x}^{T}M\textbf{x}\leq \lambda_k\sum_{i=1}^{n}a_i^2$ and $R_M(x)\leq \lambda_k$. In particular $\lambda_k$ is obtained when $\textbf{x}=a_k\textbf{v}_k$. Thus $\lambda_k \geq \min_{k\text{-dim} V}\max_{\textbf{x}\in V\setminus\{\textbf{0}\}}\frac{\textbf{x}^T M\textbf{x}}{\textbf{x}^T\textbf{x}}$.

For the other direction, let $V$ be any $k$-dim space. Let $S$ be the span of $\textbf{v}_k,\cdots,\textbf{v}_{n}$. Then $\exists \textbf{x}\in V\cap S, \textbf{x}\neq \textbf{0}$. Since $\textbf{x}=\sum_{i=k}^{n}b_i\textbf{v}_i, R_M(\textbf{x})\geq \lambda_k$.
\end{proof}

\begin{remark}
Several facts can be immediately obtained from Theorem~\ref{thm:var_char}:
\begin{enumerate}
    \item If $\lambda_1$ is the smallest eigenvalue of a real symmetric matrix $M$, then $\lambda_1=\min_{\textbf{x}\neq \textbf{0}}R_M(\textbf{x})$. Furthermore, every minimizer is an eigenvector of $\lambda_1$.
    \item If $\lambda_n$ is the largest eigenvalue of a real symmetric matrix $M$, then $\lambda_n=\max_{\textbf{x}\neq \textbf{0}}R_{M}(\textbf{x})$. Furthermore, every maximizer is an eigenvector of $\lambda_n$.
    \item If $\lambda_1$ is the smallest eigenvalue of a real symmetric matrix $M$, and $\textbf{x}_1$ is an eigenvector of $\lambda_1$, then $\lambda_2=\min_{\textbf{x}\neq \textbf{0}, \textbf{x}\perp\textbf{x}_1}R_M(\textbf{x})$. Furthermore, every minimizer is an eigenvector of $\lambda_2$.
\end{enumerate}
\end{remark}

\section{Introduction}

\subsection{Laplacian Matrices}

A graph $G$ is specified by its vertex set $V$ and edge set $E$. Given an undirected graph $G=(V, E)$, the approach of spectral graph theory is to associate a symmetric real-valued matrix to $G$, and to relate the eigenvalues of the matrix to combinatorial properties of $G$.

The most natural matrix associated to $G$ is the weighted adjacency matrix $A$ such that $A_{ij}=A_{ji}> 0$ if $(i, j)\in E$ and $A_{ij}=0$ otherwise. If $A_{ij}$'s are constant for $(i, j)\in E$, we consider the graphs to be unweighted. Otherwise, they are considered to be weighted. Note that unless otherwise specified, the edges are unique up to symmetry in $E$, i.e. $(i, j)$ and $(j, i)$ are considered to be the same edge in $E$. We define the \textbf{Laplacian matrix} of $G$ to be 
\begin{equation}
L=D-A,
\end{equation}
where the degree matrix $D$ is a diagonal matrix with $D_{ii}=A_i\textbf{1}$.

The Laplacian matrix gives rise to a useful quadratic form associated with a graph:
\begin{equation}
    \textbf{x}^{T}L\textbf{x} = \sum_{(u, v)\in E}A_{uv}(x_u-x_v)^2.
\end{equation}
If we consider $\textbf{x}$ to be a function $\textbf{x}:V\rightarrow\mathbb{R}$, then this form measures the smoothness of the function and the evaluated result is small if $\textbf{x}$ does not change much over any edge (with large positive weights). From the perspective of graph cut, if $\textbf{x}\in\{0, 1\}^{V}$ is a Boolean vector representing a cut in the graph and $A_{ij}=A_{ji}=1, \forall (i, j)\in E$, then the evaluated result counts the number of edges that cross the cut. It's also quite often that we consider instead normalized Laplacian matrices $D^{-1}L$ or $D^{-\frac{1}{2}}LD^{-\frac{1}{2}}$.

\begin{remark}[Properties of Laplacian Matrices]
Laplacian matrices have several properties:
\begin{enumerate}
    \item They are positive semidefinite. It follows that their eigenvalues are non-negative.
    \item They are additive. For a graph $G=(V, E)$, denote its Laplacian matrix by $L_G$. An edge $(i, j)$ with $i, j\in V$ can be viewed as a graph $\Delta G=(V, \{(i, j)\})$. Let $\tilde{G}$ be the graph obtained by adding the edge $(i, j)$ to $G$, we have $L_{\tilde{G}}=L_G+L_{\Delta G}$.
    \item Their eigenvalues are invariant under graph isomorphism. If we permute the vertices then the eigenvectors are similarly permuted. Let $P$ be a permutation matrix, then
    \begin{equation}
        L\textbf{v} = \lambda \textbf{v} \text{ iff } (PLP^{T})(P\textbf{v}) = \lambda P\textbf{v}.
    \end{equation}
\end{enumerate}
\end{remark}

\begin{proof}
\hfill
\begin{enumerate}
    \item That Laplacian matrices are positive semi-definite comes from the quadratic form we derived above. The existence of eigenvalues is justified by the real Spectral Theorem. Since the matrices are positive semi-definite, the eigenvalues are non-negative. 
    \item This can be verified by applying the definition.
    \item Note that $P^{T}P=I$.
\end{enumerate}
\end{proof}

Below We prove several relations between the Laplacian matrices and certain purely combinatorial properties of the corresponding graphs. We will start with unnormalized Laplacian matrices and then extend the results to the normalized cases.

\begin{theorem} Let $G=(V, E)$ be a weighted undirected graph and denote its unnormalized Laplacian matrix by $L$. Let $\lambda_1\leq \lambda_2\leq \cdots\leq \lambda_n$ be the real eigenvalues of $L$ with multiplicities in nondecreasing order. Then
\begin{enumerate}
    \item $\lambda_1=0$.
    \item $\lambda_k=0$ iff $G$ has at least $k$ connected components.
    \item Let $D$ denote the degree matrix of $G$ and let $D_{\max}=\max_{v\in V}D_{vv}$. Then $\lambda_n\leq 2D_{\max}$. In particular, the equality holds when all nodes have the same degree (i.e. the graph is a $D_{\max}$-regular graph) and at least one of the connected components of $G$ is bipartite.
\end{enumerate}
Note that the second result suggests that the multiplicity of $0$ as an eigenvalue is precisely the number of connected components of $G$.
\end{theorem}

\begin{proof}
\hfill
\begin{enumerate}
    \item By the variational characterization of eigenvalues, we have
    \begin{equation}\lambda_1=\min_{\textbf{x}\in\mathbb{R}^{n}\setminus\{\textbf{0}\}}R_L(\textbf{x})=\min_{\textbf{x}\in\mathbb{R}^{n}\setminus\{\textbf{0}\}}\frac{\sum_{(u, v)\in E}A_{uv}(x_u-x_v)^2}{\sum_{v\in V}x_v^2}\geq 0.\end{equation} If we take $\textbf{x}$ to be an all-one vector $\textbf{1}=(1,\cdots, 1)$, then $R_L(\textbf{x})=0$. Hence $\lambda_1=0$.

    \item By the variational characterization of eigenvalues, we have
    \begin{equation}\lambda_k=\min_{k-\text{dim}S}\max_{\textbf{x}\in S\setminus\{\textbf{0}\}}R_{L}(\textbf{x})\end{equation} If $\lambda_k=0$, then $\exists$ $S$ a $k$-dim subspace such that $\forall \textbf{x}\in S$, it is constant within each connected component. Therefore the graph has at least $k$ connected components.
    
    Conversely, assume $G$ has at least $k$ connected components. Let $\textbf{1}^{(i)}\in\mathbb{R}^{n}$ be a vector such that $\textbf{1}^{(i)}_v=1$ if $v$ belongs to the $i$-th connected component and $0$ otherwise. Consider $S$ to be the span of $\textbf{1}^{(1)}, \cdots, \textbf{1}^{(k)}$, then $R_{L}(\textbf{x})=0$ $\forall \textbf{x}\in S$. Hence $\lambda_k=0$.

    \item \begin{align}\textbf{x}^{T}L\textbf{x}&=\sum_{(u, v)\in E}A_{uv}(x_u-x_v)^2=\sum_{v\in V}D_{vv}x_v^2 - 2\sum_{(u, v)\in E}A_{uv}x_u x_v\\
    &= 2\sum_{v\in V}D_{vv}x_v^2 - (\sum_{v\in V}D_{vv}x_v^2 + 2\sum_{(u, v)\in E}A_{uv}x_u x_v)\\
    &= 2\sum_{v\in V}D_{vv}x_v^2 - \sum_{(u, v)\in E}A_{uv}(x_u+x_v)^2\\
    &\leq 2D_{\max}\sum_{v\in V}x_v^2 - \sum_{(u, v)\in E}A_{uv}(x_u+x_v)^2 \end{align}
    By the variational characterization of eigenvalues, \ \begin{align}\lambda_n=\max_{\textbf{x}\in\mathbb{R}^{n}\setminus\{\textbf{0}\}}R_{L}(\textbf{x}) \leq \max_{\textbf{x}\in\mathbb{R}^{n}\setminus\{\textbf{0}\}} 2D_{\max} - \frac{\sum_{(u, v)\in E}A_{uv}(x_u+x_v)^2}{\sum_{v\in V}x_v^2}\leq 2D_{\max}. \end{align}
    
    The equality holds when $D_{vv}=D_{\max}, \forall v\in V$ and $\sum_{(u, v)\in E}A_{uv}(x_u+x_v)^2=0$. Since $\textbf{x}\neq \textbf{0}$, $\exists U=\{v | v\in V, x_v=c\}$ and $V=\{v | v\in V, x_v=-c\}$ for some $c\neq 0$ s.t. the nodes in $U$ are only connected to some nodes in $V$ and vice versa. In other words, they form a connected component that is bipartite.
\end{enumerate}
\end{proof}

\begin{theorem}
Let $G=(V, E)$ be a weighted undirected graph. Let $\hat{L}=D^{-1}L$ and $\tilde{L}=D^{-\frac{1}{2}}LD^{-\frac{1}{2}}$ be its two normalized Laplacian matrices. Then
\begin{enumerate}
    \item \begin{equation} \textbf{x}^{T}\tilde{L}\textbf{x} = \sum_{(u, v)\in E} A_{uv} \left(\frac{x_u}{\sqrt{D_{uu}}}-\frac{x_v}{\sqrt{D_{vv}}}\right)^2 \end{equation}
    \item $\lambda$ is an eigenvalue of $\hat{L}$ with eigenvector $u$ iff $Lu=\lambda Du$.
    \item $\lambda$ is an eigenvalue of $\hat{L}$ with eigenvector $u$ iff $\lambda$ is an eigenvalue of $\tilde{L}$ with eigenvector $w=D^{\frac{1}{2}}u$.
    \item $0$ is an eigenvalue of $\hat{L}$ with eigenvector $\textbf{1}$ and an eigenvalue of $\tilde{L}$ with eigenvector $D^{\frac{1}{2}}\textbf{1}$.
    \item $\hat{L}$ and $\tilde{L}$ are positive semi-definite and have $n$ non-negative real-valued eigenvalues $0=\lambda_1\leq \cdots\leq \lambda_n$.
    \item The multiplicity of the eigenvalue $0$ of both $\hat{L}$ and $\tilde{L}$ equals the number of connected components in $G$.
    \item $\lambda_n\leq 2$. In particular, the equality holds when at least one of the connected components of $G$ is bipartite.
\end{enumerate}
\end{theorem}

\begin{proof}
\hfill
\begin{enumerate}
    \item \begin{align}\textbf{x}^{T}\tilde{L}\textbf{x}&=\textbf{x}^{T}(D^{-\frac{1}{2}}LD^{-\frac{1}{2}})\textbf{x}=\textbf{x}^{T}(D^{-\frac{1}{2}}(D-A)D^{-\frac{1}{2}})\textbf{x}=\textbf{x}^{T}\textbf{x}-\textbf{x}^{T}D^{-\frac{1}{2}}AD^{-\frac{1}{2}}\textbf{x}\\
    &= \textbf{x}^{T}\textbf{x} - 2\sum_{(u, v)\in E}\frac{A_{uv}}{\sqrt{D_{uu}}\sqrt{D_{vv}}}x_u x_v\\
    &= \sum_{(u, v)\in E}\frac{A_{uv}}{D_{uu}}x_u^2 + \frac{A_{uv}}{D_{vv}}x_v^2 - 2 \frac{A_{uv}}{\sqrt{D_{uu}}\sqrt{D_{vv}}}x_u x_v\\
    &= \sum_{(u, v)\in E} A_{uv} \left(\frac{x_u}{\sqrt{D_{uu}}}-\frac{x_v}{\sqrt{D_{vv}}}\right)^2\end{align}
    
    \item The verification takes a direct computation.
    
    \item \begin{align}\hat{L}u=D^{-1}Lu=\lambda u\Longleftrightarrow D^{-1}LD^{-\frac{1}{2}}w=\lambda D^{-\frac{1}{2}}w\Longleftrightarrow\tilde{L}w=\lambda w\end{align}
    
    \item $\hat{L}\textbf{1}=D^{-1}L\textbf{1}=\textbf{1}-D^{-1}A\textbf{1}=0$. Hence $\textbf{1}$ is an eigenvector of $\hat{L}$ with eigenvalue $0$. Apply the previous result we have that $D^{\frac{1}{2}}\textbf{1}$ is an eigenvector of $\tilde{L}$ with eigenvalue $0$.
    
    \item That $\tilde{L}$ is positive semi-definite is from the first result. The existence of its $n$ eigenvalues is justified by the spectral theorem. Since it's positive semi-definite, all of its eigenvalues are non-negative. By the third result, the eigenvalues of $\hat{L}$ are also non-negative, hence $\hat{L}$ is positive semi-definite. 
    
    \item We only need to prove the case for $\tilde{L}$ and the case for $\hat{L}$ will follow from the third result.
    
    By the variational characterization of eigenvalues, we have \begin{equation}\lambda_k=\min_{k-\text{dim}S}\max_{\textbf{x}\in S\setminus\{\textbf{0}\}}R_{\tilde{L}}(\textbf{x})=\min_{k-\text{dim}S}\max_{\textbf{x}\in S\setminus\{\textbf{0}\}}\frac{\sum_{(u, v)\in E} A_{uv} \left(\frac{x_u}{\sqrt{D_{uu}}}-\frac{x_v}{\sqrt{D_{vv}}}\right)^2}{\sum_{v\in V}x_v^2}\end{equation}
    
    If $\lambda_k=0$, then $\exists$ $S$ a $k$-dim subspace such that $\forall \textbf{x}\in S$, $D^{-\frac{1}{2}}\textbf{x}$ is constant within each connected component. Therefore the graph has at least $k$ connected components. 
    
    Conversely, assume $G$ has at least $k$ connected components. Let $D^{\frac{1}{2}}\textbf{1}^{(i)}\in\mathbb{R}^{n}$ be a vector such that $(D^{\frac{1}{2}}\textbf{1}^{(i)})_v=\sqrt{D_{vv}}$ if $v$ belongs to the $i$-th connected component and $0$ otherwise. Consider $S$ to be the span of $D^{\frac{1}{2}}\textbf{1}^{(1)},\cdots, D^{\frac{1}{2}}\textbf{1}^{(k)}$, then $R_{\tilde{L}}(\textbf{x})=0$ $\forall \textbf{x}\in S$. Hence $\lambda_k=0$.
    
    It follows that $\lambda_k=0$ iff $G$ has at least $k$ connected components. Hence the multiplicity of the eigenvalue $0$ of $\tilde{L}$ equals the number of connected components in $G$.
    
    \item \begin{align}\textbf{x}^{T}\tilde{L}\textbf{x} &= \sum_{(u, v)\in E}A_{uv}\left(\frac{x_u}{\sqrt{D_{uu}}}-\frac{x_v}{\sqrt{D_{vv}}}\right)^2 = \sum_{v\in V}x_v^2-2\sum_{(u, v)\in E}\frac{A_{uv}}{\sqrt{D_{uu}}\sqrt{D_{vv}}}x_u x_v\\
    &=2\sum_{v\in V}x_v^2-\left(\sum_{v\in V}x_v^2+2\sum_{(u, v)\in E}\frac{A_{uv}}{\sqrt{D_{uu}}\sqrt{D_{vv}}}x_u x_v\right)\\
    &= 2\sum_{v\in V}x_v^2-\sum_{(u, v)\in E}A_{uv}\left(\frac{x_u}{\sqrt{D_{uu}}}+\frac{x_v}{\sqrt{D_{vv}}}\right)^2\end{align}
    
    By the variational characterization of eigenvalues, 
    \begin{align}
    \lambda_n = \max_{\textbf{x}\in\mathbb{R}^{n}\setminus\{\textbf{0}\}}R_{\tilde{L}}(\textbf{x}) \leq \max_{\textbf{x}\in\mathbb{R}^{n}\setminus\{\textbf{0}\}} 2 - \frac{\sum_{(u, v)\in E}A_{uv}\left(\frac{x_u}{\sqrt{D_{uu}}}+\frac{x_v}{\sqrt{D_{vv}}}\right)^2}{\sum_{v\in V}x_v^2}\leq 2
    \end{align} The equality holds when $\sum_{(u, v)\in E}A_{uv}\left(\frac{x_u}{\sqrt{D_{uu}}}+\frac{x_v}{\sqrt{D_{vv}}}\right)^2=0$. Since $\textbf{x}\neq\textbf{0}$, $\exists U=\{v|v\in V, \frac{x_v}{\sqrt{D_{vv}}}=c\}$ and $V=\{v|v\in V, \frac{x_v}{\sqrt{D_{vv}}}=-c\}$ for some $c\neq 0$ s.t. the nodes in $U$ are only connected to some nodes in $V$, vice versa. In other words, they form a connected component that is bipartite.
\end{enumerate}
\end{proof}

\section{Fiedler Value, Edge Expansion, and Cheeger's Inequalities}

\subsection{Motivation from Graph Cut}

A graph cut is a partition of $V$ into two disjoint nonempty sets (clusters) $A$ and $B$. Intuitively, a good cut should minimize the connections between the sets and maximize the connections within each set. It then seems natural to measure the quality of a cluster by
\begin{equation}
    \sum_{u\in A}\sum_{v\in B}A_{uv}
\end{equation}
Unfortunately this metric fails to consider the connections within a set and can be minimized by some undesired trivial cuts, e.g. a cut such that one set has only one vertex.

\subsection{Edge Expansion}

To address the issue, we now consider a different metric.

If $G=(V, E)$ is an undirected graph, and $\emptyset \subset S\subseteq V$ is a set of vertices, we call the quantity 

\begin{equation}
    \phi(S) := \frac{\sum_{u\in S}\sum_{v\in V\setminus S}A_{uv}}{\sum_{u\in S}\sum_{v\in V}A_{uv}} = \frac{\sum_{u\in S}\sum_{v\in V\setminus S}A_{uv}}{\sum_{u\in S}d_u}
\end{equation}

the \textbf{edge expansion} of $S$. The quantity $\phi(S)$ is the average fraction of connections pointing outside of $S$ for a random element of $S$.

We define the expansion of a cut $(S, V\setminus S)$ as 

\begin{equation}
\phi(S, V\setminus S) := \max\{\phi(S), \phi(V\setminus S)\}=\frac{\sum_{u\in S}\sum_{v\in V\setminus S}A_{uv}}{\min\{\sum_{u\in S}d_{u}, \sum_{v\in V\setminus S}d_{v}\}}.\footnote{Here we are following the convention used in~\cite{Luca_notes_2016}. Note that it is common in the literature to use the notation $\phi(S)$ to refer to the quantity that we call $\phi(S, V\setminus S)$.}
\end{equation}

Note that it's also quite often to call $\phi(S, V\setminus S)$ as the \textbf{conductance} of $S$ or $V\setminus S$.

The edge expansion/conductance of the graph $G$ is defined as

\begin{equation}
    \phi(G) := \min_{S}\phi(S, V\setminus S) = \min_{S:1\leq |S|\leq \floor*{\frac{|V|}{2}}}\phi(S)
\end{equation}

\subsection{The Fiedler Value}

We've learned that the second smallest eigenvalue $\lambda_2$ of Laplacian matrices is $0$ iff the graph is disconnected. In fact, Miroslav Fiedler suggested considering $\lambda_2$ to be a measure of how well connected the graph is and called it the ``algebraic connectivity'' of a graph. We will follow the convention of~\cite{Spielman_notes_2018} and call this value the \textbf{Fiedler value}. Fiedler proved that the further $\lambda_2$ is from $0$, the better connected the graph is.

Finding cuts of small expansion is a problem with several applications. Unfortunately, computing the optimal cut with respect to edge expansion is NP-hard and it is an open question if there is a polynomial-time approximation with a constant-factor approximation ratio.

The following algorithm~\ref{alg:fiedler} was proposed by Fiedler, and it works well in practice when $\textbf{x}$ is the eigenvector corresponding to the Fiedler value.

\begin{algorithm}
\caption{Approximating the Optimal Graph Cut}
\begin{algorithmic}[1]
\Require{graph $G=(V, E)$, $\textbf{x}\in\mathbb{R}^{|V|}$}

\State Sort the vertices according to $\textbf{x}$ and relabel them in order as $v_1,\cdots, v_n$.
\State $k^{\ast}=\arg\min_{k}\phi(\{v_1,\cdots, v_k\},\{v_{k+1},\cdots, v_n\})$
\State Return $(\{v_1,\cdots, v_k^{\ast}\},\{v_{k^{\ast}+1},\cdots, v_n\})$
\end{algorithmic}
\label{alg:fiedler}
\end{algorithm}

The algorithm can be implemented in $O(|E|+|V|\log |V|)$. First we can sort the vertices in $O(|V|\log |V|)$. The cut of minimal expansion that respects the sorted order can be found in time $O(E)$. In order to find the cut, we only need to compute $e_k=E(\{v_1,\cdots, v_k\},\{v_{k+1},\cdots, v_n\})$ for $k=1,\cdots, n-1$. When $k=1$, this is just the degree of $v_1$ excluding its self loops. From $k$ to $k+1$, we have $e_{k+1}=e_{k}-E(\{v_{k+1}\},\{v_1,\cdots, v_k\}) + E(\{v_{k+1}\},\{v_{k+2},\cdots, v_{n}\})$. On operation, that can be done in time $O(D_{k+1,k+1})$. Thus the total running time is of order of $\sum_{v\in V}D_{vv}$, i.e. $O(|E|)$.

\subsection{Cheeger's Inequalities}

Cheeger's inequality gives a tight connection between edge expansion and the Fiedler value. 

\begin{theorem}[Cheegerâ€™s Inequalities] Let $G$ be an undirected regular graph and $\lambda_1\leq \lambda_2\leq\cdots\leq\lambda_n$ be the eigenvalues of the normalized Laplacian $\tilde{L}=D^{-\frac{1}{2}}LD^{-\frac{1}{2}}$. Then

\begin{equation}
    \frac{\lambda_2}{2}\leq \phi(G)\leq \sqrt{2\lambda_2}
\end{equation}

% Furthermore, if $(S, V\setminus S)$ is the cut found by Fiedler's algorithm given the eigenvector of $\lambda_2$, then
% 
% \begin{equation}
%    \phi(S, V\setminus S)\leq \sqrt{2\lambda_2}
% \end{equation}
% and
% \begin{equation}
%     \phi(S, V\setminus S)\leq 2\sqrt{\phi(G)}.
% \end{equation}
%
% The latter one gives a worst-case guarantee of the quality of the solution found by the algorithm.

\end{theorem}

We will start with the case of $d$-regular unweighted graphs and later extend the result to irregular graphs.

\subsubsection{The Case of $d$-regular Unweighted Graphs}

\textbf{Proof that $\frac{\lambda_2}{2}\leq \phi(G)$}

Let $S$ be a set of vertices such that $\phi(S, V\setminus S)=\phi(G)$. Note that $\forall S\subset V$, we have
\begin{equation}
    \phi(S)=\frac{E(S, V\setminus S)}{d|S|} = \frac{\sum_{u\in S}\sum_{v\in V\setminus S}(1-0)^2}{d|S|} = R_{\tilde{L}}(\textbf{1}_{S}).
\end{equation}
It follows that
\begin{align}
    R_{\tilde{L}}(\textbf{1}_{S}) &\leq \phi(G)\\
    R_{\tilde{L}}(\textbf{1}_{V\setminus S}) &\leq \phi(G)
\end{align}
From the variational characterizattion of eigenvalues, we have that
\begin{equation}
    \lambda_2=\min_{2-\text{dim} X}\max_{\textbf{x}\in X\setminus\{0\}}R_{\tilde{L}}(\textbf{x})
\end{equation}
We will prove the inequality by showing that all the vectors in the $2$-dimensional space $X$ of linear combinations of the orthogonal vectors $\textbf{1}_{S}$, $\textbf{1}_{V\setminus S}$ have Rayleigh quotient at most $2\phi(G)$. This is a consequence of the following Lemma.

\begin{lemma}
Let $\textbf{x}$ and $\textbf{y}$ be two orthogonal vectors, and let $M$ be a positive semi-definite matrix. Then 

\begin{equation}
    R_{M}(\textbf{x}+\textbf{y}) \leq 2\max\{R_{M}(\textbf{x}), R_{M}(\textbf{y})\}
\end{equation}
\end{lemma}

\begin{proof}
By the spectral theorem, $M$ has $n$ eigenvalues corresponding to $n$ eigenvectors that form an orthonormal basis. Since $M$ is positive semi-definite, the eigenvalues are non-negative. For convenience, we order them as $0\leq \lambda_1\leq \cdots \leq \lambda_n$, corresponding to eigenvectors $\textbf{v}_1,\cdots, \textbf{v}_n$. Let $\textbf{x}=\sum_{i=1}^{n}a_i\textbf{v}_i$, $\textbf{y}=\sum_{i=1}^{n}b_i\textbf{v}_i$.

\begin{align}
    R_{M}(\textbf{x} + \textbf{y}) &= \frac{\sum_{i}\lambda_i(a_i+b_i)^2}{||x+y||^2} = \frac{\sum_{i}\lambda_i(a_i+b_i)^2}{||x||^2+||y||^2} \leq \frac{\sum_{i}2\lambda_i(a_i^2+b_i^2)}{||x||^2+||y||^2} (\textbf{Cauchy-Schwarz inequality})\\
    &= \frac{2R_{M}(\textbf{x})\cdot ||\textbf{x}||^2 + 2R_{M}(\textbf{y})\cdot ||\textbf{y}||^2}{||\textbf{x}||^2+||\textbf{y}||^2} \leq 2\max\{R_{M}(\textbf{x}), R_{M}(\textbf{y})\}
\end{align}
\end{proof}

\textbf{Proof that $\phi(G)\leq \sqrt{2\lambda_2}$}

Below we will prove a stronger result.

\begin{lemma}
Let $\textbf{x}$ be a vector orthogonal to $\textbf{1}$ and let $(S, V\setminus S)$ be the cut found by Fiedler's algorithm given $\textbf{x}$. Then

\begin{equation}
    \phi(S, V\setminus S)\leq \sqrt{2R_{\tilde{L}}(\textbf{x})}
\end{equation}
\end{lemma}

Assume this lemma holds, then with $\textbf{x}=\textbf{v}_2$ being the eigenvector of $\tilde{L}$ corresponding to $\lambda_2$, we have 

\begin{equation}
    \phi(G)\leq \phi(S, V\setminus S)\leq \sqrt{2R_{\tilde{L}}(\textbf{v}_2)}=\sqrt{2\lambda_2}.
\end{equation}

In practice, often we use the Fiedler's algorithm with an approximate eigenvector $\textbf{x}$. This result is useful for giving a guarantee on the quality of the solution found by the Fiedler's algorithm.

We will prove the lemma in two steps:
\begin{enumerate}
    \item We show that studying non-negative vectors with at most $\frac{|V|}{2}$ entries implies the cases of vectors orthogonal to $\textbf{1}$. Recall that $\textbf{1}$ is an eigenvector of $\tilde{L}$ corresponding to $\lambda_1$ and $\textbf{v}_2\perp\textbf{1}$.
    \item We study the cases of non-negative vectors with at most $\frac{|V|}{2}$ entries.
\end{enumerate}

Formally, these two steps correspond to the two lemmas below.

\begin{lemma}
\label{lemma:relation_to_nonnegative}
Let $\textbf{x}\in\mathbb{R}^{|V|}$ be orthogonal to $\textbf{1}$. Then $\exists$ a vector $\textbf{y}\in\mathbb{R}_{\geq 0}^{|V|}$ with at most $\frac{|V|}{2}$ non-zero entries such that

\begin{equation}
    R_{\tilde{L}}(\textbf{y}) \leq R_{\tilde{L}}(\textbf{x})
\end{equation}

Furthermore, $\forall t\in (0, \max_v y_v)$, the cut $(\{v:y_v\geq t\}, \{v:y_v<t\})$ has been considered by Fiedler's algorithm on $\textbf{x}$.
\end{lemma}

\begin{lemma}
\label{lemma:study_nonnegative}
Let $\textbf{y}\in\mathbb{R}_{\geq 0}^{|V|}$ be a vector with non-negative entries. Then $\exists$ a $t\in(0, \max_{v}y_v]$ such that

\begin{equation}
    \phi(\{v:y_v\geq t\}) \leq \sqrt{2R_{\tilde{L}}(\textbf{y})}
\end{equation}
\end{lemma}

\begin{proof}[Proof of Lemma~\ref{lemma:relation_to_nonnegative}]
\hfill

We observe that $\forall c\in\mathbb{R}$,
\begin{equation}
    R_{\tilde{L}}(\textbf{x}+c\textbf{1}) = \frac{\sum_{(u, v)\in E}(x_u-x_v)^2}{d||\textbf{x} + c\textbf{1}||^2} = \frac{\sum_{(u, v)\in E}(x_u-x_v)^2}{d(||\textbf{x}||^2+||c\textbf{1}||^2)} \leq \frac{\sum_{(u, v)\in E}(x_u-x_v)^2}{d||\textbf{x}||^2} = R_{\tilde{L}}(\textbf{x}).
\end{equation}
Let $m$ be the median value of the entries of $\textbf{x}$, call $\textbf{x}':=\textbf{x}-m\textbf{1}$. Then we have $R_{\tilde{L}}(\textbf{x}')\leq R_{\tilde{L}}(\textbf{x})$. Also the median of the entries of $\textbf{x}'$ is zero, meaning that $\textbf{x}'$ has at most $\frac{|V|}{2}$ positive entries and at most $\frac{|V|}{2}$ negative entries. We will refer to the vertices $v$ such that $x'_{v} > 0$ as the positive vertices and those with $x'_{v} < 0$ as the negative vertices.

We write $\textbf{x}'=\textbf{x}^{+} - \textbf{x}^{-}$, where $\textbf{x}^{+}_{v}=x'_v\textbf{1}[x'_v >0]$ and $\textbf{x}^{-}_{v}=-x'_{v}\textbf{1}[x'_v <0]$. Then $\textbf{x}^{+}$ and $\textbf{x}^{-}$ are orthogonal, non-negative and each of them has at most $\frac{|V|}{2}$ nonzero entries. Note that $\forall t$, the cut defined by the set $\{v:x_v^{+}\geq t\}$ is one of the cuts considered by the Fiedler's algorithm on $\textbf{x}$, because it is the cut $(\{v:x_v<t+m\}, \{v:x_v\geq t+m\})$. The case for $\textbf{x}^{-}$ is similar.

It remains to show that

\begin{equation}
    \min\{R_{\tilde{L}}(\textbf{x}^{+}), R_{\tilde{L}}(\textbf{x}^{-})\}\leq R_{\tilde{L}}(\textbf{x}') \leq R_{\tilde{L}}(\textbf{x})
\end{equation}

We claim that

\begin{align}
    R_{\tilde{L}}(\textbf{x}') &= \frac{\sum_{(u, v)\in E}(x_u-x_v)^2}{d||\textbf{x}'||^2} = \frac{\sum_{(u, v)\in E}((x_u^{+}-x_v^{+})-(x_u^{-}-x_v^{-}))^2}{d(||\textbf{x}^{+}||^2+||\textbf{x}^{-}||^2)}\\
    &\geq \frac{\sum_{(u, v)\in E}(x_u^{+}-x_v^{+})^2+(x_u^{-}-x_v^{-})^2}{d(||\textbf{x}^{+}||^2+||\textbf{x}^{-}||^2)}\\
    &= \frac{R_{\tilde{L}}(\textbf{x}^{+})\cdot ||\textbf{x}^{+}||^2 + R_{\tilde{L}}(\textbf{x}^{-})\cdot ||\textbf{x}^{-}||^2 }{d(||\textbf{x}^{+}||^2+||\textbf{x}^{-}||^2)}\\
    &\geq \min\{R_{\tilde{L}}(\textbf{x}^{+}), R_{\tilde{L}}(\textbf{x}^{-})\}
\end{align}

The only step that we need to justify is that for every edge $(u, v)$ we have 

\begin{equation}
((x_u^{+}-x_v^{+})-(x_u^{-}-x_v^{-}))^2 \geq (x_u^{+}-x_v^{+})^2+(x_u^{-}-x_v^{-})^2
\end{equation}

This clearly holds when both vertices are positive or negative. Without loss of generality, assume $u$ is positive and $v$ is negative, then 

\begin{align}
    ((x_u^{+}-x_v^{+})-(x_u^{-}-x_v^{-}))^2 &= (x_u^{+} + x_v^{+})^2 \geq (x_u^{+})^2 + (x_v^{+})^2 = (x_u^{+}-x_v^{+})^2+(x_u^{-}-x_v^{-})^2.
\end{align}

\end{proof}

Finally, we will provide a probabilistic proof for Lemma~\ref{lemma:study_nonnegative}.

\begin{proof}[Proof of Lemma~\ref{lemma:study_nonnegative}]
\hfill

Since multiplication by a scalar does not affect the Rayleigh quotient of a vector, without loss of generality, we may assume that $\max_v y_v=1$. 

We consider the probabilistic process in which we pick $t > 0$ in such a way that $t^2\sim Unif((0, 1])$ and then define the non-empty set $S_{t}:=\{v: y_v \geq t\}$.

We will prove that

\begin{equation}
    \label{equ:ratio_expectation}
    \frac{\mathbb{E}[E(S_t, V\setminus S_t)]}{\mathbb{E}[d|S_t|]}\leq \sqrt{2R_{\tilde{L}}(\textbf{y})}.
\end{equation}
\hfill

\begin{remark} 
\label{rem:probability}
Let $X$ and $Y$ be random variables such that $\mathbb{P}[Y > 0] = 1$. Then 
\begin{equation}
    \mathbb{P}\left[\frac{X}{Y}\leq \frac{\mathbb{E}[X]}{\mathbb{E}[Y]}\right] > 0
\end{equation}
Let $r:=\frac{\mathbb{E}[X]}{\mathbb{E}[Y]}$, then by linearity $\mathbb{E}[X-rY]=0$. It follows that $\mathbb{P}[X-rY\leq 0] > 0$. Since $Y > 0$ with probability $1$, $\mathbb{P}\left[\frac{X}{Y}\leq \frac{\mathbb{E}[X]}{\mathbb{E}[Y]}\right] > 0$.
\end{remark}

With the Remark~\ref{rem:probability}, if equation~\ref{equ:ratio_expectation} holds, then the Lemma~\ref{lemma:study_nonnegative} will be true. We will now prove equation~\ref{equ:ratio_expectation}.

The denominator and the numerator of the equation~\ref{equ:ratio_expectation} can be transformed as follows:

\begin{align}
    \mathbb{E}[d|S_t|] &= d\sum_{v\in V}\mathbb{P}[v\in S_t] = d\sum_{v\in V}\mathbb{P}[y_v\geq t] = d\sum_{v\in V}\mathbb{P}[y_v^2\geq t^2] = d\sum_{v\in V}y_v^2
\end{align}
\begin{align}
    \mathbb{E}[E(S_t, V\setminus S_t)] &= \sum_{(u, v)\in E}\mathbb{P}[(u,v)\text{ is cut}] = \sum_{(u, v)\in E}|y_v^2-y_u^2| = \sum_{(u, v)\in E}|y_v-y_u|\cdot (y_v+y_u)
\end{align}

Applying Cauchy-Schwarz for twice:
\begin{align}
    \sum_{(u, v)\in E}|y_v-y_u|\cdot (y_v+y_u) &\leq \sqrt{\sum_{(u, v)\in E}(y_v-y_u)^2}\cdot \sqrt{\sum_{(u, v)\in E}(y_v+y_u)^2}\\
    &\leq \sqrt{\sum_{(u, v)\in E}(y_v-y_u)^2}\cdot\sqrt{\sum_{(u, v)\in E}2y_v^2+2y_u^2}\\
    &= \sqrt{\sum_{(u, v)\in E}(y_v-y_u)^2}\cdot\sqrt{2d\sum_{v\in V}y_v^2}
\end{align}
Putting everyting together:
\begin{align}
\frac{\mathbb{E}[E(S_t, V\setminus S_t)]}{\mathbb{E}[d|S_t|]} &\leq \sqrt{2\frac{\sum_{(u, v)\in E}(y_v-y_u)^2}{d\sum_{v\in V}y_v^2}}=\sqrt{2R_{\tilde{L}}(\textbf{y})}.
\end{align}

\end{proof}

\bibliographystyle{unsrt}
\bibliography{main}

\end{document}
